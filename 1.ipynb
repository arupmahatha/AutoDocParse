{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2309001500\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2309001500/SICSY2309001500 - CI.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2309001500/SICSY2309001500 - PL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2309001500/IG3I049039A.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/.DS_Store\n",
      "[]\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - CI 2.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - CI 3.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - CI 1.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - IG PERMIT.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - HBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - MBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311002400/SICSY2311002400 - PL.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311000100\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311000100/SICSY2311000100 - MBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311000100/SICSY2311000100 - PL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311000100/SICSY2311000100 - CI.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311000100/SICSY2311000100 - IG PERMIT.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2311000100/SICSY2311000100 - HBL.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2305001300\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2305001300/SICSY2305001300 - REVISED HBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2305001300/SICSY2305001300- II PERMIT.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2305001300/SICSY2305001300 - BL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2305001300/SICSY2305001300 - ELEPHANT PL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2305001300/SICSY2305001300 - ELEPHANT CI.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296485_INV_20231023160500.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296489_INV_20231023134647.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296487_PCK_20231023170105.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296585_INV_2023102492928.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296486_INV_20231023163802.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296487_INV_20231023170103.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296485_PCK_20231023160502.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296586_PCK_2023102490935.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296586_INV_2023102490932.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296486_PCK_20231023163804.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296489_PCK_20231023134649.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296484_INV_20231023160453.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296488_INV_2023102492922.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296488_PCK_2023102492924.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296484_PCK_20231023160456.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/K2655-PL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/MES2320296585_PCK_2023102492930.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SECSY2311000601/HBL SURR MURATA 2311000601.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312001700\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312001700/SICSY2312001700 - PL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312001700/SICSY2312001700 - IT PERMIT.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312001700/SICSY2312001700 - CI.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312001700/SICSY2312001700 - REVISED MBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312001700/SICSY2312001700 - HBL.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/Packing List SINUK 002(23).pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/Commercial Invoice SINUK 002(23).pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/IE3J391928Y.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/IE3J391929N.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/IE3J391926T.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/IE3J391930U.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2310001700/SWB.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312000300\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312000300/SICSY2312000300 - HBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312000300/SICSY2312000300 - MBL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2312000300/SICSY2312000300 - IG PERMIT.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2307001600\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2307001600/SICSY2307001600 - IE PERMIT.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2307001600/SICSY2307001600 - MBL.pdf']\n",
      "\n",
      "\n",
      "FOLDER - /Users/arup/Documents/IDP/SEA_IMPORT/SICSY2308000300\n",
      "['/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2308000300/SICSY2308000300 - DO.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2308000300/SICSY2308000300  - IG PERMIT.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2308000300/SICSY2308000300 - PL.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2308000300/SICSY2308000300 - CI.pdf', '/Users/arup/Documents/IDP/SEA_IMPORT/SICSY2308000300/SICSY2308000300 - MBL.pdf']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "# Initialize the OpenAI client outside the function\n",
    "\n",
    "client = OpenAI(api_key=\"\")\n",
    "\n",
    "# Function to clean the output folder before generating new images\n",
    "def clean_output_folder(output_folder):\n",
    "    file_list = glob.glob(os.path.join(output_folder, '*'))\n",
    "    for file_path in file_list:\n",
    "        os.remove(file_path)\n",
    "\n",
    "# Function to convert PDF to images\n",
    "def convert_pdf_to_images(pdf_path, output_folder):\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    file_name_no_extension = os.path.splitext(file_name)[0]\n",
    "    \n",
    "    images = convert_from_path(pdf_path, output_folder=output_folder, fmt='png')\n",
    "\n",
    "    if len(images) == 1:  # Check if only one page in the PDF\n",
    "        image_path = os.path.join(output_folder, f\"{file_name_no_extension}_page_1.png\")\n",
    "        images[0].save(image_path, \"PNG\")\n",
    "        return [image_path]\n",
    "\n",
    "    # Save images with expected filenames based on PDF name and page number\n",
    "    image_paths = []\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f\"{file_name_no_extension}_page_{i+1}.png\")\n",
    "        image.save(image_path, \"PNG\")  # Save the image with the expected filename\n",
    "        image_paths.append(image_path)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Function to extract text from an image using pytesseract\n",
    "def extract_text_from_image(image_path):\n",
    "    if os.path.exists(image_path):\n",
    "        return pytesseract.image_to_string(image_path)\n",
    "    else:\n",
    "        print(f\"Error: File not found at path: {image_path}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to process text and classify based on regex patterns\n",
    "def process_text(extracted_text):\n",
    "    # Define regex patterns\n",
    "    invoice_pattern = r'\\b(Commercial\\sInvoice|Invoice)\\b'\n",
    "    permit_pattern = r'\\bCARGO\\sCLEARANCE\\sPERMIT\\b'\n",
    "    packaging_list_pattern = r'\\b(PACKING\\sLIST|MANIFEST)\\b'\n",
    "    bill_of_lading_pattern = r'\\bBILL\\sOF\\sLADING\\b'\n",
    "    air_waybill_pattern = r'\\bAIR\\sWAYBILL\\b'\n",
    "    booking_confirmation_pattern = r'\\bBOOKING\\sCONFIRMATION\\b'\n",
    "    delivery_order_pattern = r'\\bDELIVERY\\sORDER\\b'\n",
    "\n",
    "    \n",
    "    output_data = {}\n",
    "\n",
    "    text = \" --- \" + extracted_text + \" --- \" \n",
    "    pre_prompt = \" You are given a document text delimited by hyphens {0} Use the document text to answer the query.\".format(text)\n",
    "    output_data['PrePrompt'] = pre_prompt\n",
    "\n",
    "    if re.search(permit_pattern, extracted_text, re.IGNORECASE):\n",
    "        output_data['FileType'] = 'Permit'\n",
    "        output_data['Prompt'] = \"Output a list of values for the following keys. Values should be separated by semicolon. Keys are: Permit Number, Message Type, Declaration Type, Importer details, Validity period, Exporter details, Handling Agent, Port of Loading/Next port of call, Port of Discharge/Final Port of call, Country of final destination, IN TRANSPORT IDENTIFIER, OU TRANSPORT IDENTIFIER, Outward carrier agent, Inward carrier agent, Conveyance reference Number, OBL/MAWB NO, ARRIVAL DATE, DEPARTURE DATE, CERTIFICATE No, PLACE OF RELEASE, PLACE OF RECEIPT, LICENCE NO, CUSTOMS PROCEDURE CODE (CPC), HS codes of all items, IN HAWB/HUCR/HBL of all items, OUT HAWB/HUCR/HBL of all items, Quantities of all items, Invoice number, Job number, Ref number, Name of company, Declarant name, Declarant code. For a key for which no value is extracted - return value as ZZZZZZ. Strictly only output a list of values for all the given keys separated by semicolon\"\n",
    "    elif re.search(air_waybill_pattern, extracted_text, re.IGNORECASE):  \n",
    "        output_data['FileType'] = 'Air Way Bill'\n",
    "        output_data['Prompt'] = \"Output a list of values for the following keys. Values should be separated by semicolon. Keys are:  Airwaybill / AWB number, Shipper name, Consignee name, Issuer details, Shipper account number, Consignee account number, Agent IATA code, Departure airport, Destination airport, Declared value, Invoice number, Invoice date, Sb number, Sb date, HS code / HSN code, Weight, Dimensions/measurements, Hawb / hbl number,Payment terms, Items/goods description. For a key for which no value is extracted - return value as ZZZZZZ. Strictly only output a list of values for all the given keys separated by semicolon\"\n",
    "    elif re.search(bill_of_lading_pattern, extracted_text, re.IGNORECASE):\n",
    "        output_data['FileType'] = 'Bill Of Lading'\n",
    "        output_data['Prompt'] = \"Output a list of values for the following keys. Values should be separated by semicolon. Keys are:  Bill of Lading Number, Shipper Details, Consignee Details, Agent details (Logistics partner), Port of Loading, Port of Discharge, Ocean Vessel number/name, Voy. no, Number of pkgs, Items/goods description, weight, Measurement/dimensions, HS code / HSN code, Invoice number, Payment terms, Date of shipping, Place of issue, Date of issue. For a key for which no value is extracted - return value as ZZZZZZ. Strictly only output a list of values for all the given keys separated by semicolon\"\n",
    "    elif re.search(invoice_pattern, extracted_text, re.IGNORECASE):\n",
    "        output_data['FileType'] = 'Invoice'\n",
    "        output_data['Prompt'] = \"Output a list of values for the following keys. Values should be separated by semicolon. Keys are: Customer details, Ship To / destination, Invoice Number, date of invoice, payment terms, Currency amount, Items, HS codes, BL number/Bill of lading number, Weight. For a key for which no value is extracted - return value as ZZZZZZ. Strictly only output a list of values for all the given keys separated by semicolon\"\n",
    "    elif re.search(packaging_list_pattern, extracted_text, re.IGNORECASE):\n",
    "        output_data['FileType'] = 'Packaging List'\n",
    "        output_data['Prompt'] = \"Output a list of values for the following keys. Values should be separated by semicolon. Keys are:  Invoice Number, Items, No of cartons, dimensions. For a key for which no value is extracted - return value as ZZZZZZ. Strictly only output a list of values for all the given keys separated by semicolon\"\n",
    "    else:\n",
    "        output_data['FileType'] = 'Unknown'\n",
    "    return output_data\n",
    "\n",
    "\n",
    "# Function to analyze images using OpenAI GPT\n",
    "def analyze_images(openai_client, prompt, pre_prompt):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": pre_prompt,\n",
    "                }\n",
    "\n",
    "            ]\n",
    "            },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return [resp.message.content for resp in response.choices]\n",
    "\n",
    "# Function to process PDF, extract text, classify, and further process using GPT\n",
    "max_token_limit = 32000\n",
    "def process_pdf(pdf_path, openai_client):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    images = convert_pdf_to_images(pdf_path, output_image_folder)\n",
    "    \n",
    "    extracted_text = \"\"\n",
    "    for image_path in images:\n",
    "        extracted_text += extract_text_from_image(image_path)\n",
    "\n",
    "    # Check if the total token count exceeds the limit\n",
    "    if len(extracted_text.split()) > max_token_limit:\n",
    "        print(f\"Skipping document {os.path.basename(pdf_path)} due to excessive length.\")\n",
    "        return None\n",
    "\n",
    "    classified_data = process_text(extracted_text)\n",
    "\n",
    "    # Only check permits, AWB and Invoice\n",
    "    check_types = ['Permit', 'Bill Of Lading', 'Packaging List', 'Invoice']\n",
    "    if classified_data['FileType'] in check_types:\n",
    "        extracted_content = analyze_images(openai_client, classified_data['Prompt'], classified_data['PrePrompt'])\n",
    "        classified_data['GPT_Response'] = extracted_content\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        classified_data['Extracted_text'] = extracted_text\n",
    "        classified_data['FileName'] = os.path.basename(pdf_path)\n",
    "        classified_data['ProcessingTime'] = processing_time\n",
    "\n",
    "        return classified_data\n",
    "    else:\n",
    "        print(\"Not Evaluating file {0} - type {1}\".format(os.path.basename(pdf_path), classified_data['FileType']))\n",
    "        return None\n",
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def create_key_value_pairs(data, file_type, file_name):\n",
    "    # Predefined keys for different document types\n",
    "    predefined_keys = {\n",
    "        'Invoice': ['Customer details', 'Ship To / destination', 'Invoice Number', 'date of invoice', 'payment terms', 'Currency', 'Items', 'HS codes','BL number/Bill of lading number','Weight'],\n",
    "        'Permit': ['Permit Number', 'Message Type', 'Declaration Type', 'Importer details', 'Validity period', 'Exporter details', 'Handling Agent', 'Port of Loading/Next port of call', 'Port of Discharge/Final Port of call', 'Country of final destination', 'IN TRANSPORT IDENTIFIER', 'OU TRANSPORT IDENTIFIER', 'Outward carrier agent', 'Inward carrier agent', 'Conveyance reference Number', 'OBL/MAWB NO', 'ARRIVAL DATE', 'DEPARTURE DATE', 'CERTIFICATE No', 'PLACE OF RELEASE', 'PLACE OF RECEIPT', 'LICENCE NO', 'CUSTOMS PROCEDURE CODE (CPC)', 'HS codes of all items', 'IN HAWB/HUCR/HBL of all items', 'OUT HAWB/HUCR/HBL of all items', 'Quantities of all items', 'Invoice number', 'Job number', 'Ref number', 'Name of company', 'Declarant name', 'Declarant code'],\n",
    "        'Packaging List': ['Invoice Number', 'Items', 'No of cartons', 'dimensions'],\n",
    "        'Bill Of Lading': ['Bill of Lading Number', 'Shipper Details', 'Consignee Details', 'Agent details (Logistics partner)', 'Port of Loading', 'Port of Discharge', 'Ocean Vessel number/name', 'Voy. no', 'Number of pkgs', 'Items/goods description', 'weight', 'Measurement/dimensions', 'HS code / HSN code', 'Invoice number', 'Payment terms', 'Date of shipping', 'Place of issue', 'Date of issue'],\n",
    "        'Air Way Bill': ['Airwaybill / AWB number', 'Shipper name', 'Consignee name', 'Issuer details', 'Shipper account number', 'Consignee account number', 'Agent IATA code', 'Departure airport', 'Destination airport', 'Declared value', 'Invoice number', 'Invoice date', 'Sb number', 'Sb date', 'HS code / HSN code', 'Weight', 'Dimensions/measurements','Hawb / hbl number','Payment terms','Items/goods description'],\n",
    "    }\n",
    "\n",
    "    # Prepare data as a dictionary for writing to CSV\n",
    "    file_data = {'File Name': file_name}\n",
    "\n",
    "    # Split the data string into values based on semicolons\n",
    "    values = data.split(';')\n",
    "\n",
    "    # Determine the predefined keys for the file type\n",
    "    extracted_keys = predefined_keys.get(file_type, [])\n",
    "\n",
    "    # Iterate over the extracted keys and values\n",
    "    for i, key in enumerate(extracted_keys):\n",
    "        value = values[i] if i < len(values) else 'NA'  # Get value or set 'NA' if not found\n",
    "        file_data[key] = value\n",
    "\n",
    "    return file_data\n",
    "\n",
    "# Function to analyze permit and add additional fields\n",
    "def analyze_permit(extracted_data, extracted_text):\n",
    "    if extracted_data['IN HAWB/HUCR/HBL of all items'] == 'ZZZZZZ':\n",
    "        hs_codes = []\n",
    "        hawb_numbers = []\n",
    "        j = 0\n",
    "        # Some files do not have HAWB/HUCR/HBL Number\n",
    "        # This flag is used to check if the number is present\n",
    "        hawb_exists = False\n",
    "        text = extracted_text\n",
    "        # Split the text into lines and remove empty lines\n",
    "        text = text.split('\\n')\n",
    "        text = [line for line in text if line != '']\n",
    "        for li,line in enumerate(text):\n",
    "            # Check if HAWB/HUCR/HBL Number is present\n",
    "            if line.startswith('IN HAWB/HUCR/HBL'):\n",
    "                hawb_exists = True\n",
    "            try:\n",
    "                # Change the range to accomodate more HS Codes\n",
    "                for i in range(j,10):\n",
    "                    if (line.split()[0] == '0'+str(i)):\n",
    "                        hs_codes.append(line.split()[1])\n",
    "                        # To avoid duplicates\n",
    "                        j = i+1\n",
    "                        if hawb_exists:\n",
    "                            hawb_numbers.append(text[li+2])\n",
    "            except:\n",
    "                pass\n",
    "            if line == 'TRADERâ€™ S REMARKS':\n",
    "                pattern = re.compile(r'INV(?:OICE)?[ #]*:?[ \\t]*([A-Za-z0-9/-]+)')\n",
    "                matches = re.findall(pattern, text[li+1])\n",
    "                if matches:\n",
    "                    extracted_data['Invoice Number'] = text[li+1].split()[-1]\n",
    "                else:\n",
    "                    extracted_data['Invoice Number'] = None\n",
    "        # Extract unique values\n",
    "        hs_codes = list(set(hs_codes))\n",
    "        hawb_numbers = list(set(hawb_numbers))\n",
    "        extracted_data['HS codes of all items'] = hs_codes\n",
    "        extracted_data['OUT HAWB/HUCR/HBL of all items'] = hawb_numbers\n",
    "    return extracted_data\n",
    "\n",
    "def analyze_AWB(extracted_data,extracted_text):\n",
    "    if extracted_data['Airwaybill / AWB number'] == \"ZZZZZZ\":\n",
    "        # Define the regex pattern\n",
    "        patterns = [\n",
    "            r'\\b\\d{3}-\\d{8}\\b',                                      # Pattern 1\n",
    "            r'\\b\\d{3}\\. \\| [A-Z]+ \\| \\d{4} \\d{4}\\b',                 # Pattern 2\n",
    "            r'\\b\\d{3} DEL\\| \\d{4}-\\d{4}\\b',                          # Pattern 3\n",
    "            r'\\b\\d{3} DEL\\| \\d{4} [A-Z]{3}-\\d{4}\\b',                 # Pattern 4\n",
    "            r'\\b\\d{3} \\d{8}\\b',                                      # Pattern 5\n",
    "            r'\\b\\d{11}\\b',                                           # Pattern 6\n",
    "            r'\\b\\d{3} \\|DEL\\| \\d{4}-\\d{4}\\b'\n",
    "        ]\n",
    "        matched_hawb = \"\"\n",
    "        # Extract AWB numbers using regex\n",
    "        for pattern_idx, pattern in enumerate(patterns, start=1):\n",
    "            match = re.search(pattern, extracted_text)\n",
    "            if match:\n",
    "                matched_hawb = match.group()\n",
    "                break\n",
    "        if matched_hawb == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            # Extract numbers only\n",
    "            matched_hawb = re.sub(r'\\D', '', matched_hawb)\n",
    "            extracted_data['Airwaybill / AWB number'] = matched_hawb\n",
    "    return extracted_data  \n",
    "\n",
    "def save_data_to_csv(data, file_type, file_name,extracted_text,output_directory):\n",
    "    output_file = os.path.join(output_directory, f'{file_type}_Output.csv')\n",
    "\n",
    "    # Create key-value pairs from the data, including the file name\n",
    "    file_data = create_key_value_pairs(data, file_type, file_name)\n",
    "\n",
    "    if file_type == \"Permit\":\n",
    "        file_data = analyze_permit(file_data,extracted_text)\n",
    "    elif file_type == \"Air Way Bill\":\n",
    "        file_data = analyze_AWB(file_data,extracted_text)\n",
    "    print(\"Final Data:\", file_data)\n",
    "\n",
    "    # Check if the file already exists, if not create a new file and write the data\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    with open(output_file, 'a', newline='') as csvfile:\n",
    "        fieldnames = list(file_data.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(file_data)\n",
    "\n",
    "# Dictionary to hold classified data for different document types\n",
    "classified_data_by_type = {\n",
    "    'Invoice': [],\n",
    "    'Permit': [],\n",
    "    'Packaging List': [],\n",
    "    'Bill Of Lading': [],\n",
    "    'Air Way Bill': [],\n",
    "}\n",
    "\n",
    "# Dictionary to store processing times for each document type\n",
    "processing_times_by_type = {\n",
    "    'Invoice': [],\n",
    "    'Permit': [],\n",
    "    'Packaging List': [],\n",
    "    'Bill Of Lading': [],\n",
    "    'Air Way Bill': [],\n",
    "}\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "# Creating the JOBS folder\n",
    "job_folder = '/Users/arup/Documents/IDP/SEA_IMPORT'\n",
    "output_folder = 'output_import/'\n",
    "pdf_file_folders = os.listdir(job_folder)\n",
    "output_image_folder = 'output_images/'\n",
    "# Iterate over the folders in the JOBS folder\n",
    "for folder in pdf_file_folders:\n",
    "    pdf_folder = os.path.join(job_folder,folder)\n",
    "    pdf_files = glob.glob(pdf_folder+'/*pdf')\n",
    "    print(\"\\n\\nFOLDER - {}\".format(pdf_folder))\n",
    "    print(pdf_files)\n",
    "    # Create an output folder for each folder in the JOBS folder\n",
    "    output_sub_folder = os.path.join(output_folder,folder)\n",
    "    if not os.path.exists(output_sub_folder):\n",
    "        os.makedirs(output_sub_folder)\n",
    "    else:\n",
    "        continue\n",
    "    # Iterate over the PDF files in the folder\n",
    "    for pdf_file in pdf_files:\n",
    "        print(pdf_file)\n",
    "\n",
    "        clean_output_folder(output_image_folder)  # Clean the output folder before processing new PDF\n",
    "\n",
    "        result = process_pdf(pdf_file, client)\n",
    "        if result:\n",
    "            extracted_text = result['Extracted_text']\n",
    "            all_outputs.append(result)\n",
    "\n",
    "            file_type = result['FileType']\n",
    "            gpt_responses = result['GPT_Response']  # Get the list of GPT responses\n",
    "            for response in gpt_responses:\n",
    "                save_data_to_csv(response, file_type, os.path.splitext(os.path.basename(pdf_file))[0],extracted_text,output_sub_folder)\n",
    "                print(f\"CSV data for {os.path.basename(pdf_file)} saved for type: {file_type}\")\n",
    "\n",
    "            # Append data to respective document type in the dictionary\n",
    "            classified_data_by_type[file_type].append(result['GPT_Response'])\n",
    "            processing_times_by_type[file_type].append(result['ProcessingTime'])\n",
    "\n",
    "    # Calculate and display average processing time for each document type\n",
    "    for doc_type, times_list in processing_times_by_type.items():\n",
    "        if times_list:\n",
    "            avg_time = sum(times_list) / len(times_list)\n",
    "            print(f\"Average Processing Time for {doc_type}: {avg_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
